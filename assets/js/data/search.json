[ { "title": "Failing to automate - GitLab merge requests", "url": "/posts/failing-to-automate-part-1/", "categories": "Automation", "tags": "gitlab, automation, bash, git", "date": "2022-11-06 15:00:00 +0300", "snippet": "Hello friend, after a long time, I am back with a new blog post. This time, I am getting sidetracked again and am writing about something different. I am going to talk about Gitlab and how I automa...", "content": "Hello friend, after a long time, I am back with a new blog post. This time, I am getting sidetracked again and am writing about something different. I am going to talk about Gitlab and how I automated the little annoyance of creating merge requests, and finally how I truly learned two of the most common jokes in our industry are deeply rooted in truth.First of all, Git is amazing. When I initially started learning about Git, it felt more like a chore I have to do. After I started using it, I realized how amazing it is. It is the closest thing to a time machine we developers get (redux coughs). I started using Gitlab in my company, and I am lovin‚Äô it. It was a very smooth transition from Github to Gitlab. I am not going to talk about the differences between the two, but I will say that I am very happy with Gitlab.The problem I had was not GitLab specifically, far from it actually. Following the trunk based development model, I create a new branch for every new feature I am working on. And sometimes, I have to create a new branch for every small change I am making. So, I have a lot of branches. And I have to create merge requests all of time. Every time I want to push my changes to remote, I have to first change my branch to dev, pull from remote, change my branch to the branch I am working on, and merge to the dev branch, and finally push to remote. Then, I have to go to Gitlab, create a merge request, select the source and target branch, and assign the right person. Luckly, if you authenticate GitLab using SSH, it provides a link to directly create a merge request. But again, you have to select the target branch, and assign the right person manually. It is a mild amount of annoyance, but if you have to do that 5 times a day, it adds up.At some point a few weeks back, I finally broke. I missed a few steps here and there, and it was causing issues either during merge, or worse, in staging environments. I decided to automate the process. I wanted to write a script that does all the song and dance of merging locally and creating a merge request with the right assignee and to the right branch.I started looking at the link GitLab shot out onto my terminal to look for hints on how to do just that. The link, once url decoded, basically looks like this https://gitlab.com/0xecho/private_repo/merge_requests/new?merge_request[source_branch]=notMain. What may directly catch you eye may be the merge_request[source_branch]=notMain. This is the branch you would currently want to merge. I went to the GitLab url, poped open DevTools and clicked Create Merge Request. I looked at the network tab, and Voila! I found the request that GitLab sends to create a merge request. It looks like this:{ \"merge_request\": { \"source_branch\": \"notMain\", \"target_branch\": \"dev\", \"title\": \"Merge notMain to dev\", \"assignee_ids\": [ 123456 ] }}Great Success! We now know what the data being sent to the server looks like, we can test the obvious hypothesis to make here: CAN WE SET THE target_branch AND assignee_ids TO WHATEVER WE WANT? The answer is unsurprisingly, yes. We can set both of them to whatever we want to the initial GET request and everything works as expected. In my excitement, I wrote a script that would create a merge request with the right assignee and to the right branch.#!/bin/bashSOURCE_BRANCH=$(git rev-parse --abbrev-ref HEAD)TARGET_BRANCH=\"dev\"ASSIGNEE_ID=\"123456\"echo \"To create a merge request from $SOURCE_BRANCH to $TARGET_BRANCH, go to:\"echo \"https://gitlab.com/0xecho/private_repo/merge_requests/new?merge_request[source_branch]=\"$SOURCE_BRANCH\"&amp;merge_request[target_branch]=\"$TARGET_BRANCH\"&amp;merge_request[assignee_ids][]=\"$ASSIGNEE_IDI was hardcoding all the values, but atleast I was able to shave of that switching branches and assigning people manually. Hurrah! Little did I know, that was just the beginning of the rabbithole.The next thing I wanted to automate was the switch, pull, switch, merge. As long I my working tree was clean, I can switch to any branch I want, pull and return to the branch I came from with no fear of having a merge conflict or uncommitted changes. I modified the script so that would do just that.#!/bin/bashSOURCE_BRANCH=$(git rev-parse --abbrev-ref HEAD)TARGET_BRANCH=\"dev\"ASSIGNEE_ID=\"123456\"git checkout $TARGET_BRANCHgit pullgit checkout $SOURCE_BRANCHgit merge $TARGET_BRANCHecho \"To create a merge request from $SOURCE_BRANCH to $TARGET_BRANCH, go to:\"echo \"https://gitlab.com/0xecho/private_repo/merge_requests/new?merge_request[source_branch]=\"$SOURCE_BRANCH\"&amp;merge_request[target_branch]=\"$TARGET_BRANCH\"&amp;merge_request[assignee_ids][]=\"$ASSIGNEE_IDIt was working great, it got me wanting to create merge requests for the sake of it. Then after a bit I wanted more, not just because hardcoding there values was giving me nightmares from ghosts of engineers past, but because I needed to change the assignee for one MR and I had to find out manually what the assignee ID was. I know üôà so primitive. I wanted to automate that as well. I created a script that would fetch the assigneees and save it to a file, and also another file that will store the selected assignee.#!/bin/bashGITLAB_API_TOKEN=\"1234567890\"curl -s https://gitlab.com/api/v4/projects/0xecho%2Fprivate_repo/members/all -H \"PRIVATE-TOKEN: $GITLAB_API_TOKEN\" | jq -r '.[] | .name + \" \" + .id' &gt; assigneees.txt#!/bin/bashecho \"Enter the number of the assignee you want to select\"cat assigneees.txt | nl read -p \"Enter the number: \" assignee_numberassignee_id=$(cat assigneees.txt | sed -n \"$assignee_number p\" | awk '{print $2}')echo $assignee_id | tee assignee_id.txt | xargs -I {} echo \"Assignee ID set to {}\"So we can now select the assignee from a list of all the assigneees. I modified the script to read the assignee ID from the file, and the TARGET_BRANCH as an required argument.#!/bin/bashSOURCE_BRANCH=$(git rev-parse --abbrev-ref HEAD)if [ -z \"$1\" ]then echo \"Please provide the target branch as an argument\" exit 1fiTARGET_BRANCH=$1ASSIGNEE_ID=$(cat assignee_id.txt)if [ -z \"$ASSIGNEE_ID\" ]then echo \"Please set the assignee\" exit 1figit checkout $TARGET_BRANCHgit pullgit checkout $SOURCE_BRANCHgit merge $TARGET_BRANCHecho \"To create a merge request from $SOURCE_BRANCH to $TARGET_BRANCH, go togit checkout $TARGET_BRANCHgit pullgit checkout $SOURCE_BRANCHgit merge $TARGET_BRANCHecho \"To create a merge request from $SOURCE_BRANCH to $TARGET_BRANCH, go to:\"echo \"https://gitlab.com/0xecho/private_repo/merge_requests/new?merge_request[source_branch]=\"$SOURCE_BRANCH\"&amp;merge_request[target_branch]=\"$TARGET_BRANCH\"&amp;merge_request[assignee_ids][]=\"$ASSIGNEE_IDI was happy with how this little script turned out. But then I scrolled down on the GitLab docs page I was refering to. I saw this: You can use Git push options to perform certain actions for merge requests at the same time as pushing changes:. It the goes on to explain how I could have done git push -o merge_request.create -o merge_request.target=dev -o merge_request.assignee_id=123456 and it would have created the merge request for me. I was upset, I had spent so much time on this, and I could have done it in a fraction of the time. But then I realized, I had learned a quite few of things along the way, and I had created the switch, pull, switch, merge part that I still use.Programmers really do try to automate a task that can be done manually in 15 seconds, and then spend 15 hours trying to do it. Also, not reading the docs for 5 more seconds can save you from creating a ‚Äúsolution‚Äù with more flaws than a 5 year old‚Äôs drawing of a unicorn.I know this was more of a rambling than a post, but I wanted to share my experience with you guys. I hope you enjoyed reading this as much as I enjoyed writing it. I would love to hear your thoughts on this.Thank you for reading! üôè Until next week! ü§ì Goodbye friend! üëã" }, { "title": "MongoDB Optimization Part 2", "url": "/posts/mongodb-optimization-part-2/", "categories": "MongoDB, Optimization", "tags": "mongo, nosql, optimization, mongodb, mongodb-optimization, series-1", "date": "2022-10-22 09:54:00 +0300", "snippet": "On the last ‚Ä¶In the previous post, we looked at how we can benchmark our queries to get an idea of how fast they are. We also looked at how we can get the execution plan of our queries. In this pos...", "content": "On the last ‚Ä¶In the previous post, we looked at how we can benchmark our queries to get an idea of how fast they are. We also looked at how we can get the execution plan of our queries. In this post, we will create sick analogies to understand how MongoDB works and learn a bit more about indexes along the way.I will read 500 words, and I will write 500 moreIn any data storage system, no matter what type it is, we almost always have two types of operations, reads and writes. This is because we can essentially boil down any complex operation to a read or a write in respect to the data store. Sure we may transform the data in some way, chain multiple operations together, etc. But at the end of the day, we are either reading or writing data.If the only operations we have are reads and writes, then the reason our queries are not performing well must because of issues in reads or writes. As we will see later in this series, the more we try to read optimize our database, the more likely we will sacrifice our write performance, or vice versa. The need for every use case is different, and we must always be aware of the tradeoffs we are making. In this post, we will focus on read optimization.To make matters simpler, and since I‚Äôm a bit of a fan of analogies, I will try to abstract the entire database engine and database system into a clerk at a library. I know, it‚Äôs a bit of a stretch, but sue me ‚Ä¶ I‚Äôm pretty sure it won‚Äôt hold up in court, but it will (probably) help us understand the concepts better. We will also assume you are an absolute madlad armed only with the most rediculous requests, while the humble clerk is trying to help you out as much as possible.Clerk KentImagine you went to library (yes, imagine because we both know you don‚Äôt go to libraries anymore). This library you went to is a bit quirky, you can‚Äôt go past the counter, and your only point of accessing the library is through the clerk. The library also holds books that only have a single copy, so the clerk has to photocopy the entire book and hand it over to you.In this analogy, the clerk is the database engine, the library is the database, the book is the document, and the photocopy is the database request. We can get on with the rest of the post now.P.S This post won‚Äôt focus much (if not at all) on code, but rather on the concepts. I will try to explain the concepts as best as I can. I will hopefully have the benchmarked code for the examples in the next post.How reads typically work?When we go to the clerk, the first thing we do is (after greetings ofcourse) tell him the description of the book we want. But how do we do that actually? If we know exactly which book we want, we can tell the clerk the ISBN (International Standard Book Number). But I‚Äôm not sure we go around holding a list of ISBN numbers, they‚Äôre not very human readable, and communicate no useful information about the book to be used else where.What we probably do however be, asking the clerk to find a book based on a description. For example, we might say I want a book by Agatha Tolkien, written in 2069, and is about a gentleman hobbit detective. This way we can let the clerk know what book we want by using a description of the book (in a declarative way).A beginner clerk would go to the shelf and start looking for the book. He would go through every book in the library, one by one and checks if the book matches the description. If it does, he pile it in a cart or something (the mechanics of this won‚Äôt matter until much later). If it doesn‚Äôt, he moves on to the next book. When the clerk finishes checking the entire library, he would then go to the photocopy machine and photocopy the books in the cart. He would then hand over the photocopies to you.This is how a naive implementation of a read operation would work. The database engine would take the query, and go through every document in the collection, one by one, and return the documents that match the query. This is a very inefficient way of doing things, and I‚Äôm sure you can see why.The first takeaway we can have from this is that the more data we return, the more time it takes to return the data. This is because the clerk has to go through more books, and photocopy more books. This is also why we should always try to limit the amount of data we return, and only return the data we need.Common read problemsNow that we have a basic understanding of how reads work in MongoDB, we can look at some of the common problems we can encounter when reading data from MongoDB. The first problem we can encounter is the naive implementation of a read operation. The naive implementation is not a problem in itself, but it is a symptom of a bigger problem, which is that the clerk is not using any of the tools at his disposal.Indexes: What do they know? Do they know things? Let‚Äôs find out!The tools the clerk has at his disposal are the indexes. Indexes are commonly found at the back of books, and they are used to help you find occurences of a word in a book. But the concept of indexes cannot be limited to books, it can be applied to any data structure. In our library, we can have one book that is not an actual book, but a list of where books of some characteristics are located. This list is called an index.For example, if we created an index that holds the location of all the books written by the same author, it may look like this: Author Location Agatha Tolkien Room 4, Shelf 2, Row 3, Book 1 Agatha Tolkien Room 9, Shelf 1, Row 1, Book 2 Elliot Alderson Room 1, Shelf 3, Row 3, Book 7 Borat Horseman Room 2, Shelf 1, Row 1, Book 8 etc‚Ä¶Now if we give the same request I want a book by Agatha Tolkien, written in 2069, and is about a gentleman hobbit detective, the clerk checks the index first. He sees that there are two books by Agatha Tolkien, so he now only has to go to those two locations, and check if the books match the description. If they do, he photocopies them, and hands them over to you. If they don‚Äôt, he moves on to the next book.Just by adding this little index, the clerk has reduced the amount of time it takes to find the books by a lot. This is because he doesn‚Äôt have to go through every book in the library, he only has to go through the books based on the index. This is the same for MongoDB, if we create an index, the database engine can use the index to find the documents that match the query, and return them. This is a much more efficient way of doing things, and we should always try to create indexes for our queries.Indexes: The bad, and the uglyThis in itself creates new problems though. The first problem is that the clerk has to maintain the index. If a book is added to, removed from, or moved in the library, the clerk has to update the index.Also, we shouldn‚Äôt forget the index itself is a book, so it takes up space in the library. If we create too may indexes on too many fields, say for example, we create an index for books that start with ‚Äòa‚Äô, ‚Äòb‚Äô, ‚Äòc‚Äô, etc. Then we would have to maintain a lot of indexes, and the library would be filled with indexes.Not all indexes are created equalSome indexes are more useful than others. For example, while creating an index for books based on the author makes sense, creating an index for books based on the first letter of the author‚Äôs name + the second letter of the book‚Äôs name probably won‚Äôt be very useful (unless you have a VERY VERY unusual usecase).Bottom line is indexes are awesome, we should always try to create indexes for our queries to improve performance, but we not just make indexes for the sake of making indexes. We should also try to make the most useful indexes for our queries. One way to achieve this is to not use indexes at first, and when we notice queries are slow, we can then create indexes for what is common in most of the queries.Mo indexes, mo problemsAnother thing before we leave the realm of indexes is compound indexes. Compound indexes are indexes that are made up of multiple fields. For example, we can create an index for books based on the author and the year the book was written. This way, we can find books based on the author, or the year the book was written, or both. This is useful for queries that most commonly appear together. This saves us from having to go through two index books to find the books we want.The problem with compound indexes is that they take up more space than single field indexes. This is because they have to store the values of multiple fields. This is why we should only create compound indexes for queries that most commonly appear together.Excuse me, clerk. I know more than you.Final thing to note that won‚Äôt, most of the time, be a problem but might want to keep in mind for that extra 1% of the time is that the clerk probably has an index selection process. This means that the clerk will choose the most useful index for each query. This is useful if you have a lot of indexes, and the clerk has to decide which index to use for the query. This itself takes time (a small amount of time, but still time). If you are a regular user of the library, and know the lay of the land, you can probably tell the clerk which index to use for each query. This will save the clerk time, and you time. This is the same for MongoDB, if you know the most useful index for each query, you can tell the database engine which index to use for each query through the $hint operator.On the next‚Ä¶This is as good a place to stop as any. I was hoping to cover more topics, especially on aggregation pipelines, but I think this is already getting too long. I hope you enjoyed this post, and if you did, please like, share, comment, and subscribe. I‚Äôll be posting more articles on MongoDB, and other topics, so stay tuned!" }, { "title": "How not to center a div", "url": "/posts/how-not-to-center-a-div/", "categories": "humor, css", "tags": "css, centering, div, fun, humor", "date": "2022-10-03 03:00:00 +0300", "snippet": "How not to center a divEver struggled to center a div? Ever googled ‚Äúhow to center a div‚Äù? You probably have seen a lot of different ways to center a div. I even googled ‚Äúhow to center a div‚Äù and t...", "content": "How not to center a divEver struggled to center a div? Ever googled ‚Äúhow to center a div‚Äù? You probably have seen a lot of different ways to center a div. I even googled ‚Äúhow to center a div‚Äù and the first result was talking about 11+ ways to center a div. It is, without a doubt, one of the most common questions in web development. So, I decided to write a blog post about it. Some of the ways to center a div are good, some of them are ok, some of them are bad, some can be really funny.I will do a quick overview of the different ways to center a div and hopefully you will learn something new. Also, please excuse the bad puns, I thought writing them on paper would be tearable.The goodsTime to FlexThey say the day flex box was released, frontend developers everywhere jumped up and down in joy. I may not believe what they say, but I do believe in the power flex box gives us. We software engineers are in the trade of tradeoffs, but once every few years we get a tool that makes us question how we survived without it. Flex box is one of those tools. Flex box is a great tool to perform most of the layout tasks we need to do. It coincidentally also makes it easy to center a div.To center a div with flex, you just need to add display: flex to the parent element and justify-content: center and align-items: center to the child element. That‚Äôs it. You have centered a div. It is that easy.&lt;div class=\"parent\"&gt; &lt;div class=\"child\"&gt;I am centered&lt;/div&gt;&lt;/div&gt;.parent { display: flex;}.child { justify-content: center; align-items: center;}A Grid-y approachCSS Grid is the new kid on the block. It is the product of some absolute mad person, who came up with the idea of having a two-dimensional flex box. Flex box is great, and apart of that greatness comes the fact that it is dimensional. Lucky for us CSS Grid is also dimensional. It is a two dimesions, while flex box is one. And last time I checked, two is greater than one. I am not saying that flex box is bad, I am just saying that CSS Grid can be used in some cases where flex box can‚Äôt, or can but with much more effort.Centering a div in a grid is not much different than centering a div in flex. You just need to add display: grid to the parent element and place-items: center center to the child element. place-items is a shorthand for align-items and justify-items. It is a great shorthand, and it also makes it easy to write (so yaaay for that).&lt;div class=\"parent\"&gt; &lt;div class=\"child\"&gt;I am centered&lt;/div&gt;&lt;/div&gt;.parent { display: grid;}.child { place-items: center center;}Marginally better than the rest?I am not sure if this is the best way to center a div, but it is definitely the easiest. If you sent the width of the current element to span the entire width of the parent element, you can then use an automatic margin to center it.&lt;div class=\"parent\"&gt; &lt;div class=\"child\"&gt;I am centered&lt;/div&gt;&lt;/div&gt;.parent { width: 100%;}.child { width: 100%; margin: 0 auto;}The okaysGrandpa‚Äôs table80‚Äôs kids will remember the good old days when they used tables to center things. It was a simpler time, there were no fancy ides, global warming was not a thing, cars couldn‚Äôt yet fly, and tables were used to center things. While it may seem hacky, it is still one of the valid ways to center a div. What you essentially do is you make the parent element a table, and the child element a table cell. Then you use vertical-align: middle to center the child element.&lt;div class=\"parent\"&gt; &lt;div class=\"child\"&gt;I am centered&lt;/div&gt;&lt;/div&gt;.parent { display: table; margin: 0 auto;}.child { display: table-cell; vertical-align: middle;}The poor man‚Äôs flex‚ÄúI just want to center a text inside my div.‚Äù, I hear you say you magnanimously attentive reader. Well, you can do that too. You can just use text-align: center on the child element. It will center all of the text inside the div. It is effectively the same as using centering a div.&lt;div class=\"parent\"&gt; &lt;div class=\"child\"&gt;I am centered&lt;/div&gt;&lt;/div&gt;.parent { text-align: center;}The badseManualThere comes a time in every developer‚Äôs life when they need to make the decision to do as little thinking about a better solution as humanly possible. This is the time when you need to do the eManual. The eManual is a technique where you just set the width and height of the element to a fixed value, and then you set the margin to half of the width and height. It is a bad technique, a really really bad technique but it is a technique nonetheless.&lt;div class=\"parent\"&gt; &lt;div class=\"child\"&gt;I am centered&lt;/div&gt;&lt;/div&gt;.parent { width: 200px; height: 200px; margin: 100px 100px;}Center tagThings come and go. Some things come back, some things never go away. The &lt;center&gt; tag is one of those things that never went away. It‚Äôs been obsolete, deprecated, lost, forgotten ‚Ä¶ you name it. It is a tag that used to be used to center its contents. Please don‚Äôt use it. It is a bad idea. P.S. Even VS Code highlights it as red.&lt;center&gt; &lt;div class=\"child\"&gt;I am centered&lt;/div&gt;&lt;/center&gt;The ugliesThe math whizI am not sure why anyone would ever use absolute positioning to center a div. It is a valid method, that works if you resize the window too. That probably is one of the worst/best ways to say it is responsive. But it is a bit too verbose for my taste, plus it is not very readable for first timers.&lt;div class=\"parent\"&gt; &lt;div class=\"child\"&gt;I am centered&lt;/div&gt;&lt;/div&gt;.parent { position: relative;}.child { position: absolute; top: 50%; left: 50%; transform: translate(-50%, -50%);}The weirdsThe eager beaverI am not sure why anyone would ever use JavaScript to center a div. Maybe it is because they are eager to have just learned JavaScript and they want to use it everywhere. Maybe it is because they are lazy and they don‚Äôt want to learn CSS. Maybe it is because they are just bad at CSS. Whatever the reason, I would advise seeking help before using JavaScript to center a div.&lt;div class=\"parent\"&gt; &lt;div class=\"child\"&gt;I am centered&lt;/div&gt;&lt;/div&gt;&lt;script&gt; const parent = document.querySelector('.parent'); const child = document.querySelector('.child'); child.style.position = 'absolute'; child.style.top = (parent.offsetHeight - child.offsetHeight) / 2 + 'px'; child.style.left = (parent.offsetWidth - child.offsetWidth) / 2 + 'px';&lt;/script&gt;The just plain wrongsEyeballin‚Äô itThis is by FAR the best technique to center a div. Bravo, just bravo for whomwver came up with this method. How you may ask? It‚Äôs plain simple. Just keep adding spaces ‚Ä¶. until it looks centered enough.&lt;div class=\"parent\"&gt; &lt;div class=\"child\"&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;I am centered&lt;/div&gt;&lt;/div&gt;The Honorable MentionsFrameworksThe very essence of frameworks is to make your life easier. If someone has already solved a problem, why not use their solution? There are many frameworks that have a built to solve a lot of ‚Äúproblems‚Äù over the years, but we all secretly know, they only initially wanted their a div to be centered.Bootstrap has a class called text-center, mx-auto and my-auto, an entire library of classes that are used to center things.ConclusionFor a determined developer, there is always a way to center a div. But for a good developer, there is always one true way to center a div. And that way should be, always be, remain, and forever be their one and only way to center a div. I chose mine to be Flexbox. What is yours? Drop a comment below and let me know." }, { "title": "MongoDB Optimization Part 1", "url": "/posts/mongodb-optimization-part-1/", "categories": "MongoDB, Optimization", "tags": "mongo, nosql, optimization, mongodb, mongodb-optimization, series-1", "date": "2022-09-25 09:54:00 +0300", "snippet": "As many before me, I was introduced to database systems in university, with a relational DBMS. I learned about normalization, transactions, ACID ‚Ä¶ the list goes on. So when I first heard of this my...", "content": "As many before me, I was introduced to database systems in university, with a relational DBMS. I learned about normalization, transactions, ACID ‚Ä¶ the list goes on. So when I first heard of this mystical thing called a document database, which didn‚Äôt use SQL, which very publicly denounced normalization, I was a bit skeptical. How could a document database be better than a relational database? After all, relational databases have been around for a very long time, and have been used to build some of the most complex systems in the world. I thought it was just a fad, and that it would die out soon. But, oh boy, was I wrong. After working with mongo for a while, I have come to appreciate its simplicity and flexibility. In this post, I will try to recap a bit on database systems and start to delve into optimization tips and tricks for mongo.Relational vs document databasesRelational databases store data in tables, where each row represents an entity, and each column represents an attribute of the entity. Each table is defined by a schema, which specifies the name of the table, and the name, type, and constraints of the columns. Basically, you can visualize relational databases as a spreadsheet, where each row is a record, and each column is a field.Document databases on the other hand, store data in documents, which are similar to JSON objects. Each document represents an entity, and each field represents an attribute of the entity. The documents are stored in collections, which are similar to tables. Collections, however, do not have a schema. This means that the documents in a collection do not need to have the same fields, and the fields do not need to have the same type. In the case of document databases, you can imagine each document is a json file. and a collection is a folder.Mongo is a document database, plain and simple. To the best of my knowledge, it is by far the most popular document database out there. It is also the database I have the most experience with, so I will be focusing on mongo in this post. However, the concepts I will be talking about are applicable to other document databases as well.BenchmarkingBefore we can get started to improve performaces, we need to have a baseline to compare against. To do this, first we need to have a dataset to work with.I have went along and created two collections, Person Field Type _id ObjectId name String age Number birthdate Date hobbies Array job ObjectId Job Field Type _id ObjectId name String salary Number description String I have seeded the database with around 100,000 (133,553 to be exact) documents in the person collection, and 20,000 documents in the job collection.Now we can create a simple wrapper to the mongo client to simplify the benchmarking process, and also to include custom export of the perfomance to create graphs based on them.Lets start with planning a simple benchmarking wrapper.What it needs to do is: Accept a query we‚Äôd like to benchmark Get the execution plan for the query Run the actual query and get the execution time Return the benchmark resultsTo make the code look a bit cleaner and nearly similar to regular mongo queries, we can create a proxy object that will wrap the mongo client and return the benchmark results when we call one of the operation methods. Let‚Äôs start by defining a const that will hold the name of operations we want to benchmark.const _BENCHMARK_METHODS = [ \"aggregate\", \"count\", \"distinct\", \"find\", \"findOne\", \"findOneAndDelete\", \"findOneAndReplace\", \"findOneAndUpdate\", \"insertMany\", \"insertOne\", \"replaceOne\", \"updateMany\", \"updateOne\",];Now we can create the proxy object. We will use the Proxy object to intercept calls to the mongo client and return the benchmark results when we call one of the operation methods.const benchmarkProxy = { get: (target, property) =&gt; { // if the property is one of the benchmark methods if (_BENCHMARK_METHODS.includes(property)) { // run the benchmark here return functionThatRunsTheBenchmark; } // return the original mongo client method return target[property]; },};That‚Äôs a good skeleton for the proxy object. Now we can start filling in the details.First, we need to implement functionThatRunsTheBenchmark . This function will accept the query we want to benchmark, and return the benchmark results.const benchmarkProxy = { get: (target, property) =&gt; { // if the property is one of the benchmark methods if (_BENCHMARK_METHODS.includes(property)) { return async (query) =&gt; { // run the benchmark here and return the results // we have access to // - target: the mongo client // - property: the name of the method we are calling (eg. find) // - query: the query we are passing to the method (eg. { name: \"John\" }) }; } // return the original mongo client method return target[property]; },};Using the above skeleton, we can start implementing the benchmarking function. We can start by getting the execution plan for the query.const benchmarkProxy = { get: (target, property) =&gt; { // if the property is one of the benchmark methods if (_BENCHMARK_METHODS.includes(property)) { // run the benchmark here return async (query) =&gt; { // get the execution plan const executionPlan = await target[property](query).explain( \"executionStats\" ); return { executionPlan, }; } // return the original mongo client method return target[property]; },};Now we can run the actual query and get the execution time.const benchmarkProxy = { get: (target, property) =&gt; { // if the property is one of the benchmark methods if (_BENCHMARK_METHODS.includes(property)) { // run the benchmark here return async (query) =&gt; { // get the execution plan const executionPlan = await target[property](query).explain( \"executionStats\" ); const start = Date.now(); await target[property](query); const end = Date.now(); const executionTime = end - start; return { executionPlan, executionTime, }; } // return the original mongo client method return target[property]; },};Our benchmarking function is now looking good enough. We can now create a factory function that we can use to inject the mongo collection we want to benchmark. This will allow us to create multiple benchmarking functions for different collections.export const benchmarkFactory = (collection) =&gt; { return new Proxy(collection, benchmarkProxy);};As a final touch, lets allow the benchmarking function to accept a configuration object that will allow us to customize the benchmarking process. For example, we can allow the user to specify the number of times to run the query. We can expand on this later to allow the user to specify other options.export const benchmarkFactory = (collection, config = {}) =&gt; { const { iterations = 1 } = config; collection._config = { iterations, }; return new Proxy(collection, benchmarkProxy);};And modify the benchmarking function to use the configuration.const benchmarkProxy = { get: (target, property) =&gt; { // if the property is one of the benchmark methods if (_BENCHMARK_METHODS.includes(property)) { // run the benchmark here return async (query) =&gt; { // get the execution plan const explainerClient = await target.explain(\"executionStats\"); const executionPlan = await explainerClient[property](query); // run the query and get the execution time const elapsedTimes = []; for (let i = 0; i &lt; target._config.iterations || 1; i++) { const start = Date.now(); await target[property](query); const end = Date.now(); elapsedTimes.push(end - start); } // we can now get a better estimate of the execution time const slowestTime = Math.max(...elapsedTimes); const fastestTime = Math.min(...elapsedTimes); const averageTime = elapsedTimes.reduce((a, b) =&gt; a + b) / elapsedTimes.length; return { executionPlan, executionTime, }; } // return the original mongo client method return target[property]; },};Amazing! We now have a simple benchmarking function that we can use to benchmark our queries. We can now use this function to benchmark our queries and get the execution plan and execution time.const { MongoClient } = require(\"mongodb\");const { benchmarkFactory } = require(\"./benchmark\");const mongoUrl = \"mongodb://localhost:27017\";const client = new MongoClient(mongoUrl);await client.connect();const db = client.db(\"test\");const jobsCollection = db.collection(\"jobs\");const benchmarkedJobsCollection = benchmarkFactory(jobsCollection, { iterations: 10,});const result = await benchmarkedJobsCollection.find({});console.log(results);The above test yields the output{ \"executionPlan\": { \"queryPlanner\": { \"plannerVersion\": 1, \"namespace\": \"test.jobs\", \"indexFilterSet\": false, \"parsedQuery\": {}, \"winningPlan\": { \"stage\": \"COLLSCAN\", \"direction\": \"forward\" }, \"rejectedPlans\": [] }, \"executionStats\": { \"executionSuccess\": true, \"nReturned\": 20060, \"executionTimeMillis\": 5, \"totalKeysExamined\": 0, \"totalDocsExamined\": 20060, \"executionStages\": { \"stage\": \"COLLSCAN\", \"nReturned\": 20060, \"executionTimeMillisEstimate\": 2, \"works\": 20062, \"advanced\": 20060, \"needTime\": 1, \"needYield\": 0, \"saveState\": 20, \"restoreState\": 20, \"isEOF\": 1, \"direction\": \"forward\", \"docsExamined\": 20060 } }, \"serverInfo\": { \"host\": \"joeking-pee-cee\", \"port\": 27017, \"version\": \"4.4.5\", \"gitVersion\": \"ff5cb77101b052fa02da43b8538093486cf9b3f7\" }, \"ok\": 1, \"$clusterTime\": { \"clusterTime\": { \"$timestamp\": \"7147275551382700033\" }, \"signature\": { \"hash\": \"AAAAAAAAAAAAAAAAAAAAAAAAAAA=\", \"keyId\": 0 } }, \"operationTime\": { \"$timestamp\": \"7147275551382700033\" } }, \"executionTime\": { \"slowestTime\": 118, \"fastestTime\": 67, \"averageTime\": 76.8 }}For now, focus only on the executionTime object (at the bottom of the output). This is the execution time of the query. We can see that the slowest time is 118ms, the fastest time is 67ms and the average time is 76.8ms. We can use this information to generalize that about 20000 documents are being returned in 70-80ms. Ofcause, this is just a rough estimate and there are a lot of factors at play like the processing power of the machine, the size of the documents, etc. But this is a good start.I know, I know that is an overwhelming amount of information, plus a lot of work for a simple query. But trust me, it will pay off later in this series. We will use this information to optimize our queries.This turned out to be a long post. I will try to keep the next post shorter. In the next post, we will look at some of the common bottlenecks we can encounter when querying MongoDB and how we can avoid them.If you have any questions or comments, please leave them in the comments section below. If you found this post useful, please share it with your friends and colleagues. If you want to be notified when I publish a new post, follow me on LinkedIn where I post updates about my blog posts.Thanks for reading!" }, { "title": "Just Migrated my Blog", "url": "/posts/just-migrated-my-blog/", "categories": "Blog", "tags": "blog, jekyll, github, markdown, github-pages", "date": "2022-09-22 08:00:00 +0300", "snippet": "I just migrated my blog to GitHub Pages. Well, I actually created a new blog using VuePress and migrated that to GitHub Pages. /r/technicallythetruth. I will talk about why I did that, but before t...", "content": "I just migrated my blog to GitHub Pages. Well, I actually created a new blog using VuePress and migrated that to GitHub Pages. /r/technicallythetruth. I will talk about why I did that, but before that a little primer on what GitHub Pages is and why I chose it.What is GitHub Pages?GitHub is the most famous repository hosting service. It allows you to share projects with other developers and work on them together or separately. Gitlab is a similar service to GitHub, but perhaps is better suited if you want a self-hosted environment. Both are industry standards and are used by millions of developers.Lucky for us both GitHub and Gitlab offer a free hosting service for static websites. This means that you can host your website on GitHub or Gitlab for free. A very common misconception is that GitHub Pages can be used to host dynamic websites. This is not true. GitHub Pages is a static storage the same way GitHub is a static storage of your code.A static website is a website that is generated once and then served to the user. This means that the website is not generated on the fly for each user. This is in contrast to a dynamic website which is generated on the fly for each user, based on the user‚Äôs request and most probably by using some form of a data store.What is Markdown?Markdown is a lightweight markup language with plain text formatting syntax. It is entirely designed to be readable as-is, without looking like it has been marked up with tags or formatting instructions. It is designed so that it can be converted to HTML and many other formats with ease. It was created by John Gruber and Aaron Swartz in 2004 and released in 2006. Markdown is often used to format readme files, for writing messages in online discussion forums, and to create rich text using a plain text editor.What is SSG (Static Site Generation)?Static site generation is the process of creating a static website from a set of files. The files are usually written in a markup language like Markdown. The static site generator then takes these files and generates a static website from them. The static website is then served to the user. We can use SSGs to create blogs, documentation sites, landing pages, and many other types of websites that donot require dynamic content and can be generated once and then served to the user.Jekyll is a popular static site generator. It is written in Ruby and is used by GitHub, which itself is built on Ruby, out of the box. This means that you can write the contents of you website easily using Markdown and then use Jekyll to generate a static website from it. Jekyll is very easy to use and is very popular among developers.Why did I choose GitHub Pages?There are many reasons why I chose GitHub Pages. First of all, I have been using GitHub for a long time and I am very familiar with it. It is also very popular and is used by millions of developers.‚ÄúThat‚Äôs not good enough reason for me‚Äù I hear you say stranger, well, look at it this way. GitHub pages is much more easier to setup than GitLab. Granted you can have very advanced configuration at your finger tips on GitLab, but for a simple blog, GitHub Pages is much easier to setup. A fork and an edit is all you need to get started.Why did I move over from VuePress?VuePress is a similar static site generator to Jekyll. It is written in JavaScript and is used by Vue.js, which itself is built on JavaScript, out of the box. I built the first version of this blog using VuePress. I was very happy with it and I still am. However, I wanted to try out Jekyll and see how it compares to VuePress. A lighthouse comparison on a localhost network showed that Jekyll was a bit faster than VuePress, but more importantly, It had much better points on SEO and Accessibility.ConclusionI hope you enjoyed this article. If you have any questions, please feel free to reach out to me on LinkedIn. I will be happy to answer them.References https://pages.github.com/ https://en.wikipedia.org/wiki/GitHub https://en.wikipedia.org/wiki/GitLab https://en.wikipedia.org/wiki/Markdown https://en.wikipedia.org/wiki/Static_web_page https://en.wikipedia.org/wiki/Static_site_generator" } ]
